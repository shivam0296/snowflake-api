{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab33b524",
   "metadata": {},
   "source": [
    "# Creating a sample star schema in snowflake warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec89074",
   "metadata": {},
   "source": [
    "## Using snowflake connector to perform database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a2ccc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector as cnn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "601f3038",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.txt', 'r') as conf:\n",
    "    details = conf.readlines()\n",
    "\n",
    "details = [x.strip() for x in details]\n",
    "\n",
    "conn = cnn.connect(\n",
    "    user=details[0],\n",
    "    password=details[1],\n",
    "    account=details[2],\n",
    "    warehouse=details[3],\n",
    "    database=details[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(conn, database_name):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS {}\".format(database_name))\n",
    "    cursor.close()\n",
    "\n",
    "create_database(conn, 'star_product_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa32917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dimension_tables(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE dim_product (\n",
    "            product_id INTEGER PRIMARY KEY,\n",
    "            product_name VARCHAR(100),\n",
    "            category VARCHAR(50)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE dim_time (\n",
    "            time_id TIMESTAMP PRIMARY KEY,\n",
    "            year INTEGER,\n",
    "            month INTEGER,\n",
    "            day INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "def create_fact_table(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE fact_sales (\n",
    "            sales_id INTEGER PRIMARY KEY,\n",
    "            product_id INTEGER,\n",
    "            time_id TIMESTAMP,\n",
    "            amount DECIMAL(18, 2),\n",
    "            quantity INTEGER,\n",
    "            FOREIGN KEY (product_id) REFERENCES dim_product(product_id),\n",
    "            FOREIGN KEY (time_id) REFERENCES dim_time(time_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "create_dimension_tables(conn)\n",
    "create_fact_table(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25bb65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dim_product_data(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO dim_product (product_id, product_name, category)\n",
    "        VALUES\n",
    "            (1, 'Product A', 'Category 1'),\n",
    "            (2, 'Product B', 'Category 2')\n",
    "    \"\"\")\n",
    "    cursor.close()\n",
    "\n",
    "def insert_dim_time_data(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO dim_time (time_id, year, month, day)\n",
    "        VALUES\n",
    "            ('2024-05-07 00:00:00', 2024, 5, 7)\n",
    "    \"\"\")\n",
    "    cursor.close()\n",
    "\n",
    "def insert_fact_sales_data(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO fact_sales (sales_id, product_id, time_id, amount, quantity)\n",
    "        VALUES\n",
    "            (1, 1, '2024-05-07 00:00:00', 100.00, 5),\n",
    "            (2, 2, '2024-05-07 00:00:00', 200.00, 3)\n",
    "    \"\"\")\n",
    "    cursor.close()\n",
    "\n",
    "insert_dim_product_data(conn)\n",
    "insert_dim_time_data(conn)\n",
    "insert_fact_sales_data(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b636479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, datetime.datetime(2024, 5, 7, 0, 0), 5, Decimal('100.00'))\n",
      "(2, datetime.datetime(2024, 5, 7, 0, 0), 3, Decimal('200.00'))\n",
      "/////\n",
      "(1, 'Product A', 'Category 1')\n",
      "(2, 'Product B', 'Category 2')\n",
      "(10, 'Cheese', 'breakfast')\n",
      "(3, 'bread', 'breakfast')\n",
      "(4, 'honey', 'breakfast')\n"
     ]
    }
   ],
   "source": [
    "def query_data(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            p.product_id,\n",
    "            f.time_id,\n",
    "            f.quantity,\n",
    "            f.amount\n",
    "        FROM \n",
    "            fact_sales f\n",
    "        JOIN \n",
    "            dim_product p ON f.product_id = p.product_id\n",
    "    \"\"\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    cursor.close()\n",
    "\n",
    "def dim_table(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "    \"\"\"\n",
    "    select * from dim_product\n",
    "    \"\"\"\n",
    "    )\n",
    "    res = cursor.fetchall()\n",
    "    for r in res:\n",
    "        print(r)\n",
    "\n",
    "query_data(conn)\n",
    "print('/////')\n",
    "dim_table(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63502aaa",
   "metadata": {},
   "source": [
    "## Using data from external API to enter into our star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b22f96c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activity': 'Start a band',\n",
       " 'type': 'music',\n",
       " 'participants': 4,\n",
       " 'price': 0.3,\n",
       " 'link': '',\n",
       " 'key': '5675880',\n",
       " 'accessibility': 0.8}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to retrieve data from the external API\n",
    "\n",
    "def retrieve_data_from_api():\n",
    "    \n",
    "    response = requests.get('https://www.boredapi.com/api/activity')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print('Error: Unable to retrieve data from the API')\n",
    "        return None\n",
    "\n",
    "def insert_data_into_database(data):\n",
    "    if data:\n",
    "        cursor = conn.cursor()\n",
    "        for item in data:\n",
    "            \n",
    "            product_id = item['product_id']\n",
    "            product_name = item['product_name']\n",
    "            category = item['category']\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO dim_product (product_id, product_name, category)\n",
    "                VALUES (%s, %s, %s)\n",
    "            \"\"\", (product_id, product_name, category))\n",
    "        cursor.close()\n",
    "        conn.commit()\n",
    "        print('Data inserted successfully into the database')\n",
    "    else:\n",
    "        print('No data to insert')\n",
    "\n",
    "# api_data = retrieve_data_from_api()\n",
    "# insert_data_into_database(api_data)\n",
    "\n",
    "data = retrieve_data_from_api()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e5875",
   "metadata": {},
   "source": [
    "When dealing with millions of records, it's more efficient to use bulk insert operations rather than executing individual `INSERT` statements for each record. Snowflake supports bulk insert operations, which can significantly improve the performance of data insertion.\n",
    "\n",
    "One efficient way to insert large amounts of data into Snowflake is to use the `COPY INTO` command. This command allows you to bulk load data from various file formats (e.g., CSV, JSON, Parquet) stored in a cloud storage service (e.g., Amazon S3, Google Cloud Storage, Azure Blob Storage) directly into Snowflake tables.\n",
    "\n",
    "Here's a general approach to bulk insert data into Snowflake using the `COPY INTO` command:\n",
    "\n",
    "1. **Export Data to a File**: Export the data from your external API to a file in a supported format (e.g., CSV, JSON).\n",
    "\n",
    "2. **Upload File to Cloud Storage**: Upload the file containing the data to a cloud storage service like Amazon S3, Google Cloud Storage, or Azure Blob Storage. Make sure the file is accessible to Snowflake.\n",
    "\n",
    "3. **Execute COPY INTO Command**: Use the `COPY INTO` command in Snowflake to load the data from the file into your Snowflake table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c43d87b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_location = 's3://bucket_name/path_to_file/data.csv'\n",
    "file ='@~/sample_file.csv'\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "    COPY INTO dim_product\n",
    "    FROM '{}'          \n",
    "    FILE_FORMAT = (TYPE = CSV)\n",
    "\"\"\".format(file))\n",
    "conn.commit()\n",
    "cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9713b",
   "metadata": {},
   "source": [
    "- We define the `file_location` variable, which contains the location of the file containing the data in cloud storage (e.g., Amazon S3).\n",
    "- We execute the `COPY INTO` command to load the data from the file into the `dim_product` table in Snowflake. \n",
    "\n",
    "By using the `COPY INTO` command with bulk insert operations, you can efficiently load millions of records into your Snowflake tables without incurring the overhead of executing individual `INSERT` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ac28558",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfd535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
